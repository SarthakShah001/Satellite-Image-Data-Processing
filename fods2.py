# -*- coding: utf-8 -*-
"""Fods2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xaHIUys09S9fG_UEoFUq8lH4XgNHWQkF
"""

from google.colab import drive
drive.mount('/content/gdrive')

# import required libraries
import h5py as h5
import numpy as np
import pandas as pd
#this file I will attach in the zip and while running you can change the path accordingly as here I have used it by uploading it to the drive.
f = h5.File("/content/gdrive/MyDrive/3-1/3-1/FoDS/20170710_s2_manual_classification_data.h5", "r")
print(f)
# Get and print list of datasets within the H5 file
datasetNames = [n for n in f.keys()]
for n in datasetNames:
        print(n)
# extracting spectra data from the H5 file
#class_ids 
class_ids=f['class_ids']
class_idsData=class_ids[:]
print(class_idsData)
#class_names
class_names=f['class_names']
class_namesData=class_names[:]
print(class_namesData)
#spectra
spectra=f['spectra'] 
spectraData=spectra[:]   
print(spectraData)
#classes
classes=f['classes'] 
classesData=classes[:]  
print(classesData.shape)

# here I am coverting the data to a csv with 14 columns where first 13 being the spectra data and last column being the label (cloud[1] or water[0] )
# firstly making the corresponding list from first 1 million samples of spectra data.
temp_data=np.array(spectraData).tolist()[0:1000000]
final_data=[]
for i in range(1000000):
  x=classesData[i]
  if x==50:
    x=1
    temp_data[i].append(x)
    final_data.append(temp_data[i])
  if x==20:
    x=0
    temp_data[i].append(x)
    final_data.append(temp_data[i])

df=pd.DataFrame(final_data)
df.to_csv('data.csv', index=False)

dataset = pd.read_csv('data.csv') 
#Splitting the data into x and y , x being spectra data, y being labels
X_train = dataset.iloc[:, 0:13].values
Y_train = dataset.iloc[:, 13].values
# Feature Scaling
# Applying Z-score normalization
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size = 0.2, random_state = 42)

import time

# Importing the Keras libraries and packages
import keras
from keras.models import Sequential
from keras.layers import Dense

#from keras.layers import Dropout
from keras import regularizers

# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
#  adding l1 regulisar to prevent overfitting
classifier.add(Dense(units = 20, kernel_initializer = 'uniform', activation = 'relu', input_dim = 13, kernel_regularizer=regularizers.l1(0.001)))

# Adding the second hidden layer
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
start_time = time.time()

# Fitting the ANN to the Training set
classifier.fit(X_train, Y_train, batch_size =1024, epochs = 100, shuffle= True )

print("--- %s seconds ---" % (time.time() - start_time))

#Making predictions

# Predicting the Test set results
y_pred = classifier.predict(X_test)
y_pred=np.rint(y_pred) 

#Evaluating the model
# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(Y_test, y_pred)

from sklearn.metrics import accuracy_score
accuracy_score(Y_test,y_pred)

#now we can further use this to classify images by coverting images to spectral data